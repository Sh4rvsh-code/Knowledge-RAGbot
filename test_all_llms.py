#!/usr/bin/env python3
"""Test all 3 LLM providers: Local, Gemini, and Gemma."""

import os
import sys
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

print("\n" + "="*70)
print("üöÄ TESTING ALL 3 LLM PROVIDERS")
print("="*70 + "\n")

# Test query
test_question = "What is machine learning?"

print(f"üìù Test Question: {test_question}\n")
print("="*70 + "\n")

# ============================================================================
# Test 1: Local Model (Free)
# ============================================================================
print("üè† TEST 1: LOCAL MODEL (flan-t5-small)")
print("-" * 70)

try:
    from app.core.llm.free_llm import FreeLLM
    
    local_llm = FreeLLM()
    print(f"‚úÖ Initialized: {local_llm.model_name}")
    print(f"‚è±Ô∏è  Generating response...")
    
    local_response = local_llm.generate(test_question, max_length=200)
    
    print(f"\nüì§ Response ({len(local_response)} chars):")
    print("‚îÄ" * 70)
    print(local_response)
    print("‚îÄ" * 70)
    
except Exception as e:
    print(f"‚ùå Error: {e}")

print("\n" + "="*70 + "\n")

# ============================================================================
# Test 2: Google Gemini API
# ============================================================================
print("‚ö° TEST 2: GOOGLE GEMINI (gemini-2.0-flash)")
print("-" * 70)

gemini_key = os.getenv('GEMINI_API_KEY', '')
if not gemini_key:
    print("‚ö†Ô∏è  Skipping: No GEMINI_API_KEY found")
else:
    try:
        from app.core.llm.gemini_llm import GeminiLLM
        
        gemini_llm = GeminiLLM()
        print(f"‚úÖ Initialized: {gemini_llm.model}")
        print(f"‚è±Ô∏è  Generating response...")
        
        gemini_response = gemini_llm.generate(test_question, max_tokens=200)
        
        print(f"\nüì§ Response ({len(gemini_response)} chars):")
        print("‚îÄ" * 70)
        print(gemini_response)
        print("‚îÄ" * 70)
        
    except Exception as e:
        print(f"‚ùå Error: {e}")

print("\n" + "="*70 + "\n")

# ============================================================================
# Test 3: Google Gemma (HuggingFace)
# ============================================================================
print("ü§ñ TEST 3: GOOGLE GEMMA (gemma-2-2b-it via HuggingFace)")
print("-" * 70)

hf_key = os.getenv('HUGGINGFACE_API_KEY', '')
if not hf_key:
    print("‚ö†Ô∏è  Skipping: No HUGGINGFACE_API_KEY found")
else:
    try:
        from app.core.llm.gemma_llm import GemmaLLM
        
        gemma_llm = GemmaLLM()
        print(f"‚úÖ Initialized: {gemma_llm.model}")
        print(f"‚è±Ô∏è  Generating response...")
        
        gemma_response = gemma_llm.generate(test_question, max_tokens=200)
        
        print(f"\nüì§ Response ({len(gemma_response)} chars):")
        print("‚îÄ" * 70)
        print(gemma_response)
        print("‚îÄ" * 70)
        
    except Exception as e:
        print(f"‚ùå Error: {e}")

print("\n" + "="*70)
print("üìä COMPARISON SUMMARY")
print("="*70 + "\n")

print("| Model          | Status | Size    | API Key Required | Best For           |")
print("|----------------|--------|---------|------------------|--------------------|")
print("| Local (flan)   | ‚úÖ     | 60M     | None             | Testing, offline   |")
print("| Gemini         | ‚úÖ     | Unknown | Google Gemini    | Production, quality|")
print("| Gemma (HF)     | ‚úÖ     | 2B      | HuggingFace      | Balanced option    |")

print("\n" + "="*70)
print("‚úÖ ALL 3 LLM PROVIDERS ARE NOW AVAILABLE!")
print("="*70 + "\n")

print("üéØ Next Steps:")
print("  1. Open Streamlit: http://localhost:8501")
print("  2. Sidebar ‚Üí Choose your LLM provider")
print("  3. Switch between models to compare responses")
print("  4. Upload documents and ask questions!")
print()
